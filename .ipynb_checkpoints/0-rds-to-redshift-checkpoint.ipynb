{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-invention",
   "metadata": {},
   "source": [
    "# RDS to Redshift Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-butterfly",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-university",
   "metadata": {},
   "source": [
    "Now that we know about pulling data from RDS into S3, let's see if we can complete the loop that we've been seeing in our diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-boulder",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-guest",
   "metadata": {},
   "source": [
    "Remember that our data will begin in our RDS database, as this is the database that gets filled from users interacting with our application.  Then we'll use our ETL server (here our laptop) to copy data from our RDS onto our ETL server as a csv file, and then we'll place our data onto S3.  \n",
    "\n",
    "Then, with our data properly exported as a csv file, we can move it into redshift.  Ok, let's get going."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-tunnel",
   "metadata": {},
   "source": [
    "### Setting up our RDS Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-meaning",
   "metadata": {},
   "source": [
    "If you do not have one already, create an RDS database that is publicly accessible.  And from there, we'll need to create tables for our foursquare application by running the `migrations/create_tables.sql` file against the RDS database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-avatar",
   "metadata": {},
   "source": [
    "After running the migrations we should see the following tables in our OLTP database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-terrorism",
   "metadata": {},
   "source": [
    "> <img src=\"./oltp_tables.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-yield",
   "metadata": {},
   "source": [
    "Ok, now generally our OLTP database will get data from our users, but for the purposes of this lab, we'll need to load that data into our RDS instance.  Our data is currently located in the directory `foursquare-fullstack/data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-rwanda",
   "metadata": {},
   "source": [
    "> Remember that we can load the data with the following pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-expert",
   "metadata": {},
   "source": [
    "> `COPY table_name\n",
    "FROM 'absolute/path/to/data.csv' \n",
    "DELIMITER ',' \n",
    "CSV HEADER;`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-wound",
   "metadata": {},
   "source": [
    "For example, with the `states.csv` file in the Documents folder, we could login to our RDS shell, and load in our states data with something like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-reader",
   "metadata": {},
   "source": [
    "> <img src=\"./import-in-data.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-theta",
   "metadata": {},
   "source": [
    "The `COPY 3` tells us that three rows were successfully copied into the states table.  So copy over the `states.csv` file to the correct table, as well as the other CSV files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-vertical",
   "metadata": {},
   "source": [
    "Once all of the data is properly loaded, then we can move onto the next step which is to export data to a CSV file onto our laptop that we can then load into S3.  We'll do that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-methodology",
   "metadata": {},
   "source": [
    "### Loading data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-soccer",
   "metadata": {},
   "source": [
    "Now that we have some data in our RDS instance, it's time to load data from our OLTP in our RDS onto our ETL server so that we can then move that data into csv files to be loaded into our OLAP on redshift.  In other words, in the diagram below, our next step is the first arrow on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-staff",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-straight",
   "metadata": {},
   "source": [
    "Now when we copy the data into our CSV file, we should do so in a way so that we can ultimately load our data to our OLAP schema.  In other words, when exporting our data, we should perform ETL in SQL so that our data is lines up with our star schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-smith",
   "metadata": {},
   "source": [
    "> <img src=\"./updated_star.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-great",
   "metadata": {},
   "source": [
    "So we'll need to export our data so that it lines up to a `venues.csv`, `locations.csv`, and `categories.csv` file as illustrated above, with the header included in the csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-serial",
   "metadata": {},
   "source": [
    "> It's probably best to first practice an ordinary `SELECT` statement to make sure we are selecting the correct data, and then combine that with the correct `\\copy` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-tiffany",
   "metadata": {},
   "source": [
    "> **Note**: Remember that when exporting data, the primary key in our csv file should align with the primary key in our OLTP tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-credit",
   "metadata": {},
   "source": [
    "The first few rows of the files on your computer should look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-referral",
   "metadata": {},
   "source": [
    "* `venues.csv`\n",
    "\n",
    "```csv\n",
    "id,name,id,price,rating,likes\n",
    "53,Los Tacos Al Pastor,50,1,,\n",
    "54,Grimaldis,51,2,2,3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-technician",
   "metadata": {},
   "source": [
    "* `categories.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-apple",
   "metadata": {},
   "source": [
    "```csv\n",
    "id,name,venue_id\n",
    "68,Pizza,54\n",
    "69,Italian,54\n",
    "70,Italian,55\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-consumer",
   "metadata": {},
   "source": [
    "* `locations.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-score",
   "metadata": {},
   "source": [
    "```csv\n",
    "id,longitude,latitude,address,zipcode,city,state\n",
    "50,40.7024,-73.9875,141 Front Street,11210,New York,New York\n",
    "51,40.7024,-73.9875,1 Front Street,11210,New York,New York\n",
    "52,40.7024,-73.9875,133 Wythe Avenue,10001,New York,New York\n",
    "53,40.7024,-73.9875,237 James Street,19019,Pennsylvania,Pennsylvania\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-population",
   "metadata": {},
   "source": [
    "### Loading Data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-growth",
   "metadata": {},
   "source": [
    "Now that we have the csv files on our ETL server, it's time to load them to S3.  If you have not already, create an S3 bucket and make the bucket publicly accessible with the appropriate permissions.  \n",
    "\n",
    "Then, the next step is to use the AWS CLI to upload files from our local computer and into that S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-providence",
   "metadata": {},
   "source": [
    "> If not currently logged into the AWS CLI, we'll need to do so.  We can do so by clicking on `My Security Credentials`, as seen below.  Then create a new access key, and then use the information to login to the command line via the `aws configure` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-virus",
   "metadata": {},
   "source": [
    "> <img src=\"./security-credentials.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-strand",
   "metadata": {},
   "source": [
    "Ok, once logged in, we'll need to set the proper permissions on an AWS bucket, and then upload our data with the appropriate aws command.  \n",
    "\n",
    "> For example, when uploading our `states.csv` file in the reading we used the following command.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-oakland",
   "metadata": {},
   "source": [
    "> `aws s3 cp states.csv s3://jigsaw-sample-data/states.csv --acl public-read`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-parameter",
   "metadata": {},
   "source": [
    "We can confirm that we issued the commands correctly by visiting our s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-anthony",
   "metadata": {},
   "source": [
    "<img src=\"./uploaded-to-s3.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-columbus",
   "metadata": {},
   "source": [
    "Ok, so now we have completed the second step.  What's left is to move our data over to redshift.  Let's get to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-jacksonville",
   "metadata": {},
   "source": [
    "### Loading Data into Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "economic-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "statistical-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-saturday",
   "metadata": {},
   "source": [
    "> <img src=\"./updated_star.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-google",
   "metadata": {},
   "source": [
    "It's best to drop any pre-existing tables if we are re-using a redshift cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "collect-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE categories;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dried-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "middle-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE venues;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "terminal-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-france",
   "metadata": {},
   "source": [
    "Now let's recreate our tables in accordance with our star schema. \n",
    "\n",
    "We can start with the locations table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subtle-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_locations_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dynamic-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_locations_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broad-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-furniture",
   "metadata": {},
   "source": [
    "And then the categories table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "upper-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_categories_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "completed-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_categories_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "christian-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-blake",
   "metadata": {},
   "source": [
    "And then the venues table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rapid-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_venues_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extended-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_venues_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lightweight-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-saturday",
   "metadata": {},
   "source": [
    "After creating the tables, let's load in the data from our s3 bucket beginning with `categories`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-rendering",
   "metadata": {},
   "source": [
    "> We'll place the query for viewing the `stl_load_errors` in case it's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"select query, substring(filename,22,25) as filename,line_number as line, \n",
    "substring(colname,0,12) as column, type, position as pos, substring(raw_line,0,50) as line_text,\n",
    "substring(raw_field_value,0,15) as field_text, \n",
    "substring(err_reason,0,45) as reason\n",
    "from stl_load_errors \n",
    "order by query desc\n",
    "limit 1;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-gazette",
   "metadata": {},
   "source": [
    "Ok, get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "corresponding-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "mighty-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-blast",
   "metadata": {},
   "source": [
    "> Then we can confirm that our categories loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "combined-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM categories LIMIT 2;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "detailed-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68, 'Pizza', 54), (69, 'Italian', 54)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()\n",
    "\n",
    "# [(68, 'Pizza', 54), (69, 'Italian', 54)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-radiation",
   "metadata": {},
   "source": [
    "Next up is to load in the data for the `locations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "concerned-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "parental-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-sensitivity",
   "metadata": {},
   "source": [
    "Then let's check some of the entries in the `locations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "grave-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 40.7024, -73.9875, '141 Front Street', '11210', 'New York', 'New York'),\n",
       " (51, 40.7024, -73.9875, '1 Front Street', '11210', 'New York', 'New York')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM locations LIMIT 2;\")\n",
    "cursor.fetchall()\n",
    "# [(50, 40.7024, -73.9875, '141 Front Street', '11210', 'New York', 'New York'),\n",
    "#  (51, 40.7024, -73.9875, '1 Front Street', '11210', 'New York', 'New York')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-better",
   "metadata": {},
   "source": [
    "And finally, it's time to load in data for our venues table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "confirmed-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "comfortable-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "distant-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM venues LIMIT 2;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "emerging-japanese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(53, 'Los Tacos Al Pastor', 50, 1, None, None),\n",
       " (54, 'Grimaldis', 51, 2, 2.0, 3)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()\n",
    "# [(53, 'Los Tacos Al Pastor', 50, 1, None, None),\n",
    "#  (54, 'Grimaldis', 51, 2, 2.0, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-falls",
   "metadata": {},
   "source": [
    "And now that we've completed the cycle, it's worth at least making one query against our redshift database.  Let's find the venue name and address of `Grimaldis`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-lotus",
   "metadata": {},
   "source": [
    "> <img src=\"./updated_star.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "better-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "indian-arabic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grimaldis', '1 Front Street')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()\n",
    "\n",
    "# [('Grimaldis', '1 Front Street')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-recognition",
   "metadata": {},
   "source": [
    "There we go.  That feels good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-fields",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-twins",
   "metadata": {},
   "source": [
    "In this lesson, we saw the full cycle of beginning with our data in our OLTP, copying the transformed data over to CSV files on our ETL server, and using the AWS CLI to move the data over to S3.  From there, we then connected to our redshift database, created the necessary tables for our star schema and copied over our data from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-vietnam",
   "metadata": {},
   "source": [
    "<img src=\"./rds_ec2_s3.jpg\" width=\"70%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
